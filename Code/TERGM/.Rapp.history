?netinf
library(GERGM)
library(ergm)
citation("gergm")
citation("ergm")
citation("NetworkInference")
help("NetworkInference")
library(twitteR)#
library(ROAuth)#
library(httr)
install.packages("twitteR")
install.packages("ROAuth")
library(twitteR)#
library(ROAuth)#
library(httr)#
#
api_key <- 	"KEuTxGdTRCzO4K2xAYiqvmXnZ"#
#
api_secret <- "sQVnHGrk9ueITxGPxKhBnDhCiY8SNb3hea8rZPFPfLT93Poq66"#
#
access_token <- "628967454-RPTNraIhPiR9i7Zh2QkBTe2os4LNswcEiDxgfpf8"#
#
access_token_secret <- "E6DL60fLMOcNqZqNlrMO5txuF1aGxF9PvuNBJbah9DyNj"
?searchTwitter
# Required libraries for the scraping#
library(twitteR)#
library(ROAuth)#
library(httr)#
#
# Codes required to scrape twitter from R#
# Must have a twitter account to do this#
# Need to create a generic "application" on twitter#
api_key <- 	"KEuTxGdTRCzO4K2xAYiqvmXnZ"#
api_secret <- "sQVnHGrk9ueITxGPxKhBnDhCiY8SNb3hea8rZPFPfLT93Poq66"#
access_token <- "628967454-RPTNraIhPiR9i7Zh2QkBTe2os4LNswcEiDxgfpf8"#
access_token_secret <- "E6DL60fLMOcNqZqNlrMO5txuF1aGxF9PvuNBJbah9DyNj"#
#
# Senators Toomey and Casey#
handles <- c("@SenToomey","@SenBobCasey")#
#
# Create an empty list object in which to store the scraping results#
tweetList <- list()#
#
# Loop over handles to scrape tweets#
# first step is to figure out the length of the loop#
loopN <- length(handles)#
# take a look#
loopN#
#
# Note 1:N creates a vector containing a sequence of integers (whole numbers) from 1 through N.#
# Create loop indexes#
loopIndexes <- 1:loopN#
# take a look#
loopIndexes#
#
# Loop over all of the handles via the loop indexes#
# loop operations go within the curly braces#
for(i in loopIndexes){#
	# extract the ith handle#
	handlei <- handles[i]#
	# store scraper results for the ith handle#
	tweetList[[i]] <- searchTwitter(handlei)#
}
# Required libraries for the scraping#
library(twitteR)#
library(ROAuth)#
library(httr)#
#
# Codes required to scrape twitter from R#
# Must have a twitter account to do this#
# Need to create a generic "application" on twitter#
api_key <- 	"KEuTxGdTRCzO4K2xAYiqvmXnZ"#
api_secret <- "sQVnHGrk9ueITxGPxKhBnDhCiY8SNb3hea8rZPFPfLT93Poq66"#
access_token <- "628967454-RPTNraIhPiR9i7Zh2QkBTe2os4LNswcEiDxgfpf8"#
access_token_secret <- "E6DL60fLMOcNqZqNlrMO5txuF1aGxF9PvuNBJbah9DyNj"#
#
# open the data pipleline from twitter#
setup_twitter_oauth(api_key, api_secret, access_token, access_token_secret)#
#
# Senators Toomey and Casey#
handles <- c("@SenToomey","@SenBobCasey")#
#
# Create an empty list object in which to store the scraping results#
tweetList <- list()#
#
# Loop over handles to scrape tweets#
# first step is to figure out the length of the loop#
loopN <- length(handles)#
# take a look#
loopN#
#
# Note 1:N creates a vector containing a sequence of integers (whole numbers) from 1 through N.#
# Create loop indexes#
loopIndexes <- 1:loopN#
# take a look#
loopIndexes#
#
# Loop over all of the handles via the loop indexes#
# loop operations go within the curly braces#
for(i in loopIndexes){#
	# extract the ith handle#
	handlei <- handles[i]#
	# store scraper results for the ith handle#
	tweetList[[i]] <- searchTwitter(handlei)#
}
?setup_twitter_oauth
# Required libraries for the scraping#
library(twitteR)#
library(ROAuth)#
library(httr)#
#
# Codes required to scrape twitter from R#
# Must have a twitter account to do this#
# Need to create a generic "application" on twitter#
api_key <- 	"KEuTxGdTRCzO4K2xAYiqvmXnZ"#
api_secret <- "sQVnHGrk9ueITxGPxKhBnDhCiY8SNb3hea8rZPFPfLT93Poq66"#
access_token <- "628967454-RPTNraIhPiR9i7Zh2QkBTe2os4LNswcEiDxgfpf8"#
access_token_secret <- "E6DL60fLMOcNqZqNlrMO5txuF1aGxF9PvuNBJbah9DyNj"#
#
# open the data pipleline from twitter#
setup_twitter_oauth(api_key, api_secret, access_token, access_token_secret)#
# hit 1 when queried about setting up local file#
#
# Senators Toomey and Casey#
handles <- c("@SenToomey","@SenBobCasey")#
#
# Create an empty list object in which to store the scraping results#
tweetList <- list()#
#
# Loop over handles to scrape tweets#
# first step is to figure out the length of the loop#
loopN <- length(handles)#
# take a look#
loopN#
#
# Note 1:N creates a vector containing a sequence of integers (whole numbers) from 1 through N.#
# Create loop indexes#
loopIndexes <- 1:loopN#
# take a look#
loopIndexes#
#
# Loop over all of the handles via the loop indexes#
# loop operations go within the curly braces#
for(i in loopIndexes){#
	# extract the ith handle#
	handlei <- handles[i]#
	# store scraper results for the ith handle#
	tweetList[[i]] <- searchTwitter(handlei)#
}
tweetList[[1]]
length(tweetList[[1]])
?searchTwitter
getUser(handlei)
names(getUser(handlei))
?smappR
install.packages("smappR")
devtools::install_github("SMAPPNYU/smappR")
install.packages("rmongodb")
# A bunch of installations to run#
doInstall <- TRUE  # Change to FALSE if you don't want packages installed.#
toInstall <- c("ROAuth", "igraph", "ggplot2", "wordcloud", "devtools", "tm",#
    "R2WinBUGS", "rmongodb", "scales")#
if(doInstall){#
    install.packages(toInstall, repos = "http://cran.r-project.org")#
    library(devtools)#
    # R packages to get twitter and Facebook data#
    install_github("streamR", "pablobarbera", subdir="streamR")#
    install_github("Rfacebook", "pablobarbera", subdir="Rfacebook")#
    # smapp R package#
    install_github("smappR", "SMAPPNYU")#
}
# Required libraries for the scraping#
library(twitteR)#
library(ROAuth)#
library(httr)#
#
# Codes required to scrape twitter from R#
# Must have a twitter account to do this#
# Need to create a generic "application" on twitter#
api_key <- 	"KEuTxGdTRCzO4K2xAYiqvmXnZ"#
api_secret <- "sQVnHGrk9ueITxGPxKhBnDhCiY8SNb3hea8rZPFPfLT93Poq66"#
access_token <- "628967454-RPTNraIhPiR9i7Zh2QkBTe2os4LNswcEiDxgfpf8"#
access_token_secret <- "E6DL60fLMOcNqZqNlrMO5txuF1aGxF9PvuNBJbah9DyNj"#
#
# open the data pipleline from twitter#
setup_twitter_oauth(api_key, api_secret, access_token, access_token_secret)#
# hit 1 when queried about setting up local file#
#
# Senators Toomey and Casey#
handles <- c("SenToomey","SenBobCasey")#
#
# Create an empty list object in which to store the scraping results#
tweetList <- list()#
#
# Loop over handles to scrape tweets#
# first step is to figure out the length of the loop#
loopN <- length(handles)#
# take a look#
loopN#
#
# Note 1:N creates a vector containing a sequence of integers (whole numbers) from 1 through N.#
# Create loop indexes#
loopIndexes <- 1:loopN#
# take a look#
loopIndexes#
#
# Loop over all of the handles via the loop indexes#
# loop operations go within the curly braces#
for(i in loopIndexes){#
	# extract the ith handle#
	handlei <- handles[i]#
	# store scraper results for the ith handle, up to 1,000 most recent tweets#
	tweetList[[i]] <- userTimeline(handlei,n=1000)#
}
tweetList[[1]]
tweetList[[2]]
?userTimeline
tweetList[[2]][[1]]
tweetList[[2]][[2]]
# loop operations go within the curly braces#
for(i in loopIndexes){#
	# extract the ith handle#
	handlei <- handles[i]#
	# store scraper results for the ith handle, up to 3,200 in the month of February#
	tweetList[[i]] <- userTimeline(handlei,n=3200)#
    # Convert result to a dataset#
    tweetList[[i]] <- do.call(rbind, lapply(tweetList[[i]], function(x) x$toDataFrame()))#
}
tweetList[[1]]
names(tweetList[[1]])
tweetList[[1]][1,]
tweetList[[1]][1,"created"]
# Required libraries for the scraping#
library(twitteR)#
library(ROAuth)#
library(httr)#
#
# Codes required to scrape twitter from R#
# Must have a twitter account to do this#
# Need to create a generic "application" on twitter#
api_key <- 	"KEuTxGdTRCzO4K2xAYiqvmXnZ"#
api_secret <- "sQVnHGrk9ueITxGPxKhBnDhCiY8SNb3hea8rZPFPfLT93Poq66"#
access_token <- "628967454-RPTNraIhPiR9i7Zh2QkBTe2os4LNswcEiDxgfpf8"#
access_token_secret <- "E6DL60fLMOcNqZqNlrMO5txuF1aGxF9PvuNBJbah9DyNj"#
#
# open the data pipleline from twitter#
setup_twitter_oauth(api_key, api_secret, access_token, access_token_secret)#
# hit 1 when queried about setting up local file#
#
# Senators Toomey and Casey#
handles <- c("SenToomey","SenBobCasey")#
#
# Create an empty object in which to store the twitter data#
tweetData <- NULL#
#
# Loop over handles to scrape tweets#
# first step is to figure out the length of the loop#
loopN <- length(handles)#
# take a look#
loopN#
#
# Note 1:N creates a vector containing a sequence of integers (whole numbers) from 1 through N.#
# Create loop indexes#
loopIndexes <- 1:loopN#
# take a look#
loopIndexes#
#
# Loop over all of the handles via the loop indexes#
# loop operations go within the curly braces#
for(i in loopIndexes){#
	# extract the ith handle#
	handlei <- handles[i]#
	# store scraper results for the ith handle, up to 3,200 in the month of February#
	tweetDatai <- userTimeline(handlei,n=3200)#
    # Convert result to a dataset#
    tweetDatai <- do.call(rbind, lapply(tweetDatai, function(x) x$toDataFrame()))#
    # add the ith dataset to the existing data#
    tweetData <- rbind(tweetData,tweetDatai)#
}
# Required libraries for the scraping#
library(twitteR)#
library(ROAuth)#
library(httr)#
#
# Codes required to scrape twitter from R#
# Must have a twitter account to do this#
# Need to create a generic "application" on twitter#
api_key <- 	"KEuTxGdTRCzO4K2xAYiqvmXnZ"#
api_secret <- "sQVnHGrk9ueITxGPxKhBnDhCiY8SNb3hea8rZPFPfLT93Poq66"#
access_token <- "628967454-RPTNraIhPiR9i7Zh2QkBTe2os4LNswcEiDxgfpf8"#
access_token_secret <- "E6DL60fLMOcNqZqNlrMO5txuF1aGxF9PvuNBJbah9DyNj"#
#
# open the data pipleline from twitter#
setup_twitter_oauth(api_key, api_secret, access_token, access_token_secret)#
# hit 1 when queried about setting up local file#
#
# Senators Toomey and Casey#
handles <- c("SenToomey","SenBobCasey")#
#
# Create an empty object in which to store the twitter data#
tweetData <- NULL#
#
# Loop over handles to scrape tweets#
# first step is to figure out the length of the loop#
loopN <- length(handles)#
# take a look#
loopN#
#
# Note 1:N creates a vector containing a sequence of integers (whole numbers) from 1 through N.#
# Create loop indexes#
loopIndexes <- 1:loopN#
# take a look#
loopIndexes#
#
# Loop over all of the handles via the loop indexes#
# loop operations go within the curly braces#
for(i in loopIndexes){#
	# extract the ith handle#
	handlei <- handles[i]#
	# store scraper results for the ith handle, up to 3,200 in the month of February#
	tweetDatai <- userTimeline(handlei,n=3200)#
    # Convert result to a dataset#
    tweetDatai <- do.call(rbind, lapply(tweetDatai, function(x) x$toDataFrame()))#
    # Add a variable to store the user's name#
    tweetDatai$handle <- handlei#
    # add the ith dataset to the existing data#
    tweetData <- rbind(tweetData,tweetDatai)#
}
tweetData[1,]
table(tweetData$screenName)
tweetDatat$created > as.Date("2017-02-14")
tweetData$created > as.Date("2017-02-14")
tweetData$created
is.Date(tweetData$created)
?as.Date
# get date part of date#
tweetDate <- substr(tweetData$created,1,10)
tweetDate
tweetDate <- as.Date(tweetDate)
tweetDate
# get date part of 'created'#
tweetDate <- substr(tweetData$created,1,10)#
# convert to a date#
tweetDate <- as.Date(tweetDate)#
#
# define pre-treatment window#
preTreatmentStart <- as.Date("2017-02-01")#
preTreatmentEnd <- as.Date("2017-02-13")#
#
# define post-treatment window#
postTreatmentStart <- as.Date("2017-02-14")#
postTreatmentEnd <- as.Date("2017-02-20")#
#
# Create a variable full of NAs#
# This assures that tweets outside of the study window are discarded#
studyPhase <- rep(NA,nrow(tweetData))#
#
# flag pre treatement tweets#
preTreatment <- tweetDate >= preTreatmentStart & tweetDate <= preTreatmentEnd#
#
# flag post treatement tweets#
postTreatment <- tweetDate >= postTreatmentStart & tweetDate <= postTreatmentEnd
table(preTreatment)
table(postTreatment)
preTreatmentStart
preTreatmentStart>preTreatmentEnd
tbale(preTreatment,postTreatment)
table(preTreatment,postTreatment)
tweetDate[which(postTreatment)]
tweetDate[which(preTreatment)]
# store "pre" and "post" flags in studyPhase#
studyPhase[which(preTreatment)] <- "pre"#
studyPhase[which(postTreatment)] <- "post"
names(tweetData)
# Tabulate pre/post treatment by senator#
table(tweetData$screenName,tweetData$studyPhase)
# add study phase to tweet data#
tweetData$studyPhase <- studyPhase#
#
# Tabulate pre/post treatment by senator#
table(tweetData$screenName,tweetData$studyPhase)
table(tweetData$screenName)
# save the data#
# note, you'll need to change the file path#
save(list="tweetData",file="/Users/bbd5087/Box Sync/Box/Teaching/Next/soda308/twitterScraping/senatorTweets.RData")
# get date part of 'created'#
tweetDate <- substr(tweetData$created,1,10)#
# convert to a date#
tweetDate <- as.Date(tweetDate)#
#
# define pre-treatment window#
preTreatmentStart <- as.Date("2016-12-20")#
preTreatmentEnd <- as.Date("2017-01-19")#
#
# define post-treatment window#
postTreatmentStart <- as.Date("2017-01-20")#
postTreatmentEnd <- as.Date("2017-02-19")#
#
# Create a variable full of NAs#
# This assures that tweets outside of the study window are discarded#
studyPhase <- rep(NA,nrow(tweetData))#
#
# flag pre treatement tweets#
preTreatment <- tweetDate >= preTreatmentStart & tweetDate <= preTreatmentEnd#
#
# flag post treatement tweets#
postTreatment <- tweetDate >= postTreatmentStart & tweetDate <= postTreatmentEnd#
#
# store "pre" and "post" flags in studyPhase#
studyPhase[which(preTreatment)] <- "pre"#
studyPhase[which(postTreatment)] <- "post"#
#
# add study phase to tweet data#
tweetData$studyPhase <- studyPhase#
#
# Tabulate pre/post treatment by senator#
# note that tweets outside the study window are not counted#
table(tweetData$screenName,tweetData$studyPhase)#
#
# save the data#
# note, you'll need to change the file path#
save(list="tweetData",file="/Users/bbd5087/Box Sync/Box/Teaching/Next/soda308/twitterScraping/senatorTweets.RData")
# Let's pretend the treatment was administered on 11/8/16, and we want to compare the month before to the month after#
# get date part of 'created'#
tweetDate <- substr(tweetData$created,1,10)#
# convert to a date#
tweetDate <- as.Date(tweetDate)#
#
# define pre-treatment window#
preTreatmentStart <- as.Date("2016-10-08")#
preTreatmentEnd <- as.Date("2016-11-08")#
#
# define post-treatment window#
postTreatmentStart <- as.Date("2016-11-08")#
postTreatmentEnd <- as.Date("2016-12-09")#
#
# Create a variable full of NAs#
# This assures that tweets outside of the study window are discarded#
studyPhase <- rep(NA,nrow(tweetData))#
#
# flag pre treatement tweets#
preTreatment <- tweetDate >= preTreatmentStart & tweetDate <= preTreatmentEnd#
#
# flag post treatement tweets#
postTreatment <- tweetDate >= postTreatmentStart & tweetDate <= postTreatmentEnd#
#
# store "pre" and "post" flags in studyPhase#
studyPhase[which(preTreatment)] <- "pre"#
studyPhase[which(postTreatment)] <- "post"#
#
# add study phase to tweet data#
tweetData$studyPhase <- studyPhase#
#
# Tabulate pre/post treatment by senator#
# note that tweets outside the study window are not counted#
table(tweetData$screenName,tweetData$studyPhase)#
#
# save the data#
# note, you'll need to change the file path#
save(list="tweetData",file="/Users/bbd5087/Box Sync/Box/Teaching/Next/soda308/twitterScraping/senatorTweets.RData")
# Let's pretend the treatment was administered on 11/8/16, and we want to compare the month before to the month after#
# get date part of 'created'#
tweetDate <- substr(tweetData$created,1,10)#
# convert to a date#
tweetDate <- as.Date(tweetDate)#
#
# define pre-treatment window#
preTreatmentStart <- as.Date("2016-12-20")#
preTreatmentEnd <- as.Date("2017-01-19")#
#
# define post-treatment window#
postTreatmentStart <- as.Date("2017-01-20")#
postTreatmentEnd <- as.Date("2017-02-19")#
#
# Create a variable full of NAs#
# This assures that tweets outside of the study window are discarded#
studyPhase <- rep(NA,nrow(tweetData))#
#
# flag pre treatement tweets#
preTreatment <- tweetDate >= preTreatmentStart & tweetDate <= preTreatmentEnd#
#
# flag post treatement tweets#
postTreatment <- tweetDate >= postTreatmentStart & tweetDate <= postTreatmentEnd#
#
# store "pre" and "post" flags in studyPhase#
studyPhase[which(preTreatment)] <- "pre"#
studyPhase[which(postTreatment)] <- "post"#
#
# add study phase to tweet data#
tweetData$studyPhase <- studyPhase#
#
# Tabulate pre/post treatment by senator#
# note that tweets outside the study window are not counted#
table(tweetData$screenName,tweetData$studyPhase)#
#
# save the data#
# note, you'll need to change the file path#
save(list="tweetData",file="/Users/bbd5087/Box Sync/Box/Teaching/Next/soda308/twitterScraping/senatorTweets.RData")
table(tweetData$screenName)
300+300+300+305+250+305+25+80+75+80+240+500+425+2035+425+425
# Required libraries for the scraping#
# install.packages("twitteR")#
library(twitteR)#
# install.packages("ROAuth")#
library(ROAuth)#
# install.packages("httr")#
library(httr)#
#
# Codes required to scrape twitter from R#
# Must have a twitter account to do this#
# Need to create a generic "application" on twitter#
# do so at https://apps.twitter.com#
api_key <- 	"KEuTxGdTRCzO4K2xAYiqvmXnZ"#
api_secret <- "sQVnHGrk9ueITxGPxKhBnDhCiY8SNb3hea8rZPFPfLT93Poq66"#
access_token <- "628967454-RPTNraIhPiR9i7Zh2QkBTe2os4LNswcEiDxgfpf8"#
access_token_secret <- "E6DL60fLMOcNqZqNlrMO5txuF1aGxF9PvuNBJbah9DyNj"#
#
# open the data pipleline from twitter#
setup_twitter_oauth(api_key, api_secret, access_token, access_token_secret)#
# hit 1 when queried about setting up local file#
#
# Senators Toomey and Casey in a CSV file#
senatorData <- read.csv("/Users/bbd5087/Box Sync/Box/Teaching/Next/soda308/twitterScraping/senatorData.csv",stringsAsFactors=F)
senatorData
# Required libraries for the scraping#
# install.packages("twitteR")#
library(twitteR)#
# install.packages("ROAuth")#
library(ROAuth)#
# install.packages("httr")#
library(httr)#
#
# Codes required to scrape twitter from R#
# Must have a twitter account to do this#
# Need to create a generic "application" on twitter#
# do so at https://apps.twitter.com#
api_key <- 	"KEuTxGdTRCzO4K2xAYiqvmXnZ"#
api_secret <- "sQVnHGrk9ueITxGPxKhBnDhCiY8SNb3hea8rZPFPfLT93Poq66"#
access_token <- "628967454-RPTNraIhPiR9i7Zh2QkBTe2os4LNswcEiDxgfpf8"#
access_token_secret <- "E6DL60fLMOcNqZqNlrMO5txuF1aGxF9PvuNBJbah9DyNj"#
#
# open the data pipleline from twitter#
setup_twitter_oauth(api_key, api_secret, access_token, access_token_secret)#
# hit 1 when queried about setting up local file#
#
# Senators Toomey and Casey in a CSV file#
senatorData <- read.csv("/Users/bbd5087/Box Sync/Box/Teaching/Next/soda308/twitterScraping/senatorData.csv",stringsAsFactors=F)
names(senatorData)
senatorData
# Required libraries for the scraping#
# install.packages("twitteR")#
library(twitteR)#
# install.packages("ROAuth")#
library(ROAuth)#
# install.packages("httr")#
library(httr)#
#
# Codes required to scrape twitter from R#
# Must have a twitter account to do this#
# Need to create a generic "application" on twitter#
# do so at https://apps.twitter.com#
api_key <- 	"KEuTxGdTRCzO4K2xAYiqvmXnZ"#
api_secret <- "sQVnHGrk9ueITxGPxKhBnDhCiY8SNb3hea8rZPFPfLT93Poq66"#
access_token <- "628967454-RPTNraIhPiR9i7Zh2QkBTe2os4LNswcEiDxgfpf8"#
access_token_secret <- "E6DL60fLMOcNqZqNlrMO5txuF1aGxF9PvuNBJbah9DyNj"#
#
# open the data pipleline from twitter#
setup_twitter_oauth(api_key, api_secret, access_token, access_token_secret)#
# hit 1 when queried about setting up local file#
#
# Senators Toomey and Casey in a CSV file#
senatorData <- read.csv("/Users/bbd5087/Box Sync/Box/Teaching/Next/soda308/twitterScraping/senatorData.csv",stringsAsFactors=F)
names(senatorData)
senatorData
# Required libraries for the scraping#
# install.packages("twitteR")#
library(twitteR)#
# install.packages("ROAuth")#
library(ROAuth)#
# install.packages("httr")#
library(httr)#
#
# Codes required to scrape twitter from R#
# Must have a twitter account to do this#
# Need to create a generic "application" on twitter#
# do so at https://apps.twitter.com#
api_key <- 	"KEuTxGdTRCzO4K2xAYiqvmXnZ"#
api_secret <- "sQVnHGrk9ueITxGPxKhBnDhCiY8SNb3hea8rZPFPfLT93Poq66"#
access_token <- "628967454-RPTNraIhPiR9i7Zh2QkBTe2os4LNswcEiDxgfpf8"#
access_token_secret <- "E6DL60fLMOcNqZqNlrMO5txuF1aGxF9PvuNBJbah9DyNj"#
#
# open the data pipleline from twitter#
setup_twitter_oauth(api_key, api_secret, access_token, access_token_secret)#
# hit 1 when queried about setting up local file#
#
# Senators Toomey and Casey in a CSV file#
senatorData <- read.csv("/Users/bbd5087/Box Sync/Box/Teaching/Next/soda308/twitterScraping/senatorData.csv",stringsAsFactors=F)
senatorData
# Required libraries for the scraping#
# install.packages("twitteR")#
library(twitteR)#
# install.packages("ROAuth")#
library(ROAuth)#
# install.packages("httr")#
library(httr)#
#
# Codes required to scrape twitter from R#
# Must have a twitter account to do this#
# Need to create a generic "application" on twitter#
# do so at https://apps.twitter.com#
api_key <- 	"KEuTxGdTRCzO4K2xAYiqvmXnZ"#
api_secret <- "sQVnHGrk9ueITxGPxKhBnDhCiY8SNb3hea8rZPFPfLT93Poq66"#
access_token <- "628967454-RPTNraIhPiR9i7Zh2QkBTe2os4LNswcEiDxgfpf8"#
access_token_secret <- "E6DL60fLMOcNqZqNlrMO5txuF1aGxF9PvuNBJbah9DyNj"#
#
# open the data pipleline from twitter#
setup_twitter_oauth(api_key, api_secret, access_token, access_token_secret)#
# hit 1 when queried about setting up local file#
#
# Senators Toomey and Casey in a CSV file#
senatorData <- read.csv("/Users/bbd5087/Box Sync/Box/Teaching/Next/soda308/twitterScraping/senatorData.csv",stringsAsFactors=F)
senatorData
# Required libraries for the scraping#
# install.packages("twitteR")#
library(twitteR)#
# install.packages("ROAuth")#
library(ROAuth)#
# install.packages("httr")#
library(httr)#
#
# Codes required to scrape twitter from R#
# Must have a twitter account to do this#
# Need to create a generic "application" on twitter#
# do so at https://apps.twitter.com#
api_key <- 	"KEuTxGdTRCzO4K2xAYiqvmXnZ"#
api_secret <- "sQVnHGrk9ueITxGPxKhBnDhCiY8SNb3hea8rZPFPfLT93Poq66"#
access_token <- "628967454-RPTNraIhPiR9i7Zh2QkBTe2os4LNswcEiDxgfpf8"#
access_token_secret <- "E6DL60fLMOcNqZqNlrMO5txuF1aGxF9PvuNBJbah9DyNj"#
#
# open the data pipleline from twitter#
setup_twitter_oauth(api_key, api_secret, access_token, access_token_secret)#
# hit 1 when queried about setting up local file#
#
# Senators Toomey and Casey in a CSV file#
senatorData <- read.csv("/Users/bbd5087/Box Sync/Box/Teaching/Next/soda308/twitterScraping/senatorData.csv",stringsAsFactors=F)
senatorData
# Required libraries for the scraping#
# install.packages("twitteR")#
library(twitteR)#
# install.packages("ROAuth")#
library(ROAuth)#
# install.packages("httr")#
library(httr)#
#
# Codes required to scrape twitter from R#
# Must have a twitter account to do this#
# Need to create a generic "application" on twitter#
# do so at https://apps.twitter.com#
api_key <- 	"KEuTxGdTRCzO4K2xAYiqvmXnZ"#
api_secret <- "sQVnHGrk9ueITxGPxKhBnDhCiY8SNb3hea8rZPFPfLT93Poq66"#
access_token <- "628967454-RPTNraIhPiR9i7Zh2QkBTe2os4LNswcEiDxgfpf8"#
access_token_secret <- "E6DL60fLMOcNqZqNlrMO5txuF1aGxF9PvuNBJbah9DyNj"#
#
# open the data pipleline from twitter#
setup_twitter_oauth(api_key, api_secret, access_token, access_token_secret)#
# hit 1 when queried about setting up local file#
#
# Senators Toomey and Casey in a CSV file#
senatorData <- read.csv("/Users/bbd5087/Box Sync/Box/Teaching/Next/soda308/twitterScraping/senatorData.csv",stringsAsFactors=F)#
#
# Extract just the handles#
handles <- senatorData$handle#
#
# Create an empty object in which to store the twitter data#
tweetData <- NULL#
#
# Loop over handles to scrape tweets#
# first step is to figure out the length of the loop#
loopN <- length(handles)#
# take a look#
loopN#
#
# Note 1:N creates a vector containing a sequence of integers (whole numbers) from 1 through N.#
# Create loop indexes#
loopIndexes <- 1:loopN#
# take a look#
loopIndexes#
#
# Loop over all of the handles via the loop indexes#
# loop operations go within the curly braces#
for(i in loopIndexes){#
	# extract the ith handle#
	handlei <- handles[i]#
	# store scraper results for the ith handle, up to 3,200 in the month of February#
	tweetDatai <- userTimeline(handlei,n=3200)#
    # Convert result to a dataset#
    tweetDatai <- do.call(rbind, lapply(tweetDatai, function(x) x$toDataFrame()))#
    # add the ith dataset to the existing data#
    tweetData <- rbind(tweetData,tweetDatai)#
}#
#
# Let's pretend the treatment was administered on 11/8/16, and we want to compare the month before to the month after#
# get date part of 'created'#
tweetDate <- substr(tweetData$created,1,10)#
# convert to a date#
tweetDate <- as.Date(tweetDate)#
#
# define pre-treatment window#
preTreatmentStart <- as.Date("2016-12-20")#
preTreatmentEnd <- as.Date("2017-01-19")#
#
# define post-treatment window#
postTreatmentStart <- as.Date("2017-01-20")#
postTreatmentEnd <- as.Date("2017-02-19")#
#
# Create a variable full of NAs#
# This assures that tweets outside of the study window are discarded#
studyPhase <- rep(NA,nrow(tweetData))#
#
# flag pre treatement tweets#
preTreatment <- tweetDate >= preTreatmentStart & tweetDate <= preTreatmentEnd#
#
# flag post treatement tweets#
postTreatment <- tweetDate >= postTreatmentStart & tweetDate <= postTreatmentEnd#
#
# store "pre" and "post" flags in studyPhase#
studyPhase[which(preTreatment)] <- "pre"#
studyPhase[which(postTreatment)] <- "post"#
#
# add study phase to tweet data#
tweetData$studyPhase <- studyPhase#
#
# Tabulate pre/post treatment by senator#
# note that tweets outside the study window are not counted#
table(tweetData$screenName,tweetData$studyPhase)#
#
# save the data#
# note, you'll need to change the file path#
save(list="tweetData",file="/Users/bbd5087/Box Sync/Box/Teaching/Next/soda308/twitterScraping/senatorTweets.RData")
library(amen)
ciation("amen")
citation("amen")
7*20*12
7*20*12*3
load("/Users/bbd5087/Box Sync/Box/Research/FDI_IGERT_H/Code/fdi_models/model_12_w.rda")
rm(list=ls())
load("/Users/bbd5087/Box Sync/Box/Research/FDI_IGERT_H/Code/fdi_models/model_12_w.rda")
library(ergm)
ls()
mcmc.diagnostics("fit.01.2")
fit.01.2
mcmc.diagnostics(fit.01.2)
load("/Users/bbd5087/Box Sync/Box/Research/FDI_IGERT_H/Code/fdi_models/model_04_w.rda")
ls()
mcmc.diagnostics(fit.01.2)
names(mcmc.diagnostics(fit.01.2))
names(fit.01.2)
summary(fit.01.2)
devtools::install_github("https://github.com/leifeld/btergm")
devtools::install_github("leifeld/btergm")
library(btergm)
?gofmethods
## Not run: #
# First, create data and fit a TERGM...#
networks <- list()#
for(i in 1:10){            # create 10 random networks with 10 actors#
  mat <- matrix(rbinom(100, 1, .25), nrow = 10, ncol = 10)#
  diag(mat) <- 0           # loops are excluded#
  nw <- network(mat)       # create network object#
  networks[[i]] <- nw      # add network to the list#
}#
#
covariates <- list()#
for (i in 1:10) {          # create 10 matrices as covariate#
  mat <- matrix(rnorm(100), nrow = 10, ncol = 10)#
  covariates[[i]] <- mat   # add matrix to the list#
}#
#
fit <- btergm(networks ~ edges + istar(2) +#
    edgecov(covariates), R = 100)#
#
# Then assess the goodness of fit:#
g <- gof(fit, statistics = c(triad.directed, esp, maxmod.modularity, #
    rocpr), nsim = 50)#
g#
plot(g)  # see ?"gof-plot" for details
?gof-plot
?"gof-plot"
?gofstatistics
?"gof-plot"
?gof
g <- gof(fit, statistics = c(triad.directed, esp, maxmod.modularity, #
+     rocpr), nsim = 50,joint=T)
g <- gof(fit, statistics = c(triad.directed, esp, maxmod.modularity, rocpr), nsim = 50,joint=T)
g
?ergm.terms
rnorm(5)%*%t(rnorm(5))
load("/Users/bbd5087/Downloads/match-all.RData")
rm(list=ls())
load("/Users/bbd5087/Downloads/match-all.RData")
ls()
names(x)
100/6
install.packages("GERGM")
library(GERGM)
library(GERGM)#
set.seed(12345)#
data("lending_2005")#
data("covariate_data_2005")#
data("net_exports_2005")
lending_2005
library(foreign)#
library(sna)#
#
maoz <- read.dta("/Users/bbd5087/Dropbox/professional/Research/Published/ERGM/Data/jcr2006/jcr2006replication.dta")#
maoz.vars <- c("year","statea","stateb","newmid20","dichmid20","midspline2","midspline3","minreg302","caprat301","distance","integcorrse")#
maoz.dat <- NULL#
for(i in maoz.vars){#
	maoz.dat <- cbind(maoz.dat,maoz[,which(names(maoz)==i)])#
}#
#
maoz.dat <- data.frame(maoz.dat)#
names(maoz.dat) <- maoz.vars#
#
maoz.dat <- na.omit(maoz.dat)#
#
maozrep <- glm(dichmid20~midspline2+minreg302+caprat301+distance+atopsecorr+igosecorr+tradesecorr, data=maoz.dat,family=binomial)#
#
# Create triadic changestats#
#
tchange <- numeric(nrow(maoz.dat))#
tschange <- numeric(nrow(maoz.dat))#
#
for(i in 1:nrow(maoz.dat)){#
	dat1 <- subset(maoz.dat,maoz.dat$year==maoz.dat$year[i])#
	dat1mid <- subset(dat1,dat1$dichmid20==1)#
	datc1 <- subset(dat1mid,as.numeric(dat1mid$statea==maoz.dat$statea[i])+as.numeric(dat1mid$stateb==maoz.dat$statea[i]) > 0)#
	datc2 <- subset(dat1mid,as.numeric(dat1mid$statea==maoz.dat$stateb[i])+as.numeric(dat1mid$stateb==maoz.dat$stateb[i]) > 0)#
	us1 <- unique(c(datc1$statea,datc1$stateb))#
	if(is.element(maoz.dat$statea[i],us1)) us1 <- us1[-which(us1==maoz.dat$statea[i])]#
	if(is.element(maoz.dat$stateb[i],us1)) us1 <- us1[-which(us1==maoz.dat$stateb[i])]#
	us2 <- unique(c(datc2$statea,datc2$stateb))#
	if(is.element(maoz.dat$stateb[i],us2)) us2 <- us2[-which(us2==maoz.dat$stateb[i])]#
	if(is.element(maoz.dat$statea[i],us2)) us2 <- us2[-which(us2==maoz.dat$statea[i])]#
	if(min(c(length(us1),length(us2)))>0){#
		tchange[i] <- length(intersect(us1,us2))#
		tschange[i] <- length(us1)+length(us2)#
	}#
}#
#
### Make Lagged MID#
#
# Code in maoz.dat#
#
mdcode <- paste(maoz.dat$year,"#",maoz.dat$statea,"#",maoz.dat$stateb,sep="")#
#
# mids data#
#
mids <- subset(maoz.dat,maoz.dat$dichmid20==1)#
#
lmidsc <- paste(mids$year+1,"#",mids$statea,"#",mids$stateb,sep="")#
#
lmidsc2 <- paste(mids$year+1,"#",mids$stateb,"#",mids$statea,sep="")#
#
mm1 <- match(mdcode,lmidsc)#
#
mm2 <- match(mdcode,lmidsc2) #
#
lmid <- 1-as.numeric(is.na(mm1)+is.na(mm2) ==2)#
#
whichab <- function(x,y){#
#
return(which(y==x))#
#
}#
##### Bootstrap year-by-year, with and without lagged conflict #
#
maoz.dat <- cbind(maoz.dat,tchange,tschange,lmid)#
#
m <- 1000#
set.seed(10)#
uyr <- unique(maoz.dat$year)#
coef.triad <- NULL#
coef.triad.l <- NULL#
for(i in 1:1000){#
	yrsi <- sample(uyr,length(uyr),rep=T)#
	yrsil <- as.list(yrsi)#
	inds <- unlist(lapply(yrsil,whichab,y=maoz.dat$year))#
	datai <- maoz.dat[inds,]#
	mod.triad <- glm(dichmid20~midspline3+minreg302+caprat301+distance+integcorrse+tchange+tschange, data=datai,family=binomial)#
	mod.triadl <- glm(dichmid20~midspline3+minreg302+caprat301+distance+integcorrse+tchange+tschange+lmid, data=datai,family=binomial)#
	coef.triad <- rbind(coef.triad,rbind(mod.triad$coefficients))#
	coef.triad.l <- rbind(coef.triad.l,rbind(mod.triadl$coefficients))#
}#
#
dput(list(coef=coef.triad,coefl = coef.triad.l),"/Users/bbd5087/Dropbox/professional/Research/Published/ERGM/Data/bootergmDep.txt")
names(maoz)
library(foreign)#
library(sna)#
#
maoz <- read.dta("/Users/bbd5087/Dropbox/professional/Research/Published/ERGM/Data/jcr2006/jcr2006replication.dta")#
maoz.vars <- c("year","statea","stateb","newmid20","dichmid20","midspline2","midspline3","minreg302","caprat301","distance","integcorrse","atopsecorr")#
maoz.dat <- NULL#
for(i in maoz.vars){#
	maoz.dat <- cbind(maoz.dat,maoz[,which(names(maoz)==i)])#
}#
#
maoz.dat <- data.frame(maoz.dat)#
names(maoz.dat) <- maoz.vars#
#
maoz.dat <- na.omit(maoz.dat)#
#
maozrep <- glm(dichmid20~midspline2+minreg302+caprat301+distance+atopsecorr+igosecorr+tradesecorr, data=maoz.dat,family=binomial)#
#
# Create triadic changestats#
#
tchange <- numeric(nrow(maoz.dat))#
tschange <- numeric(nrow(maoz.dat))#
#
for(i in 1:nrow(maoz.dat)){#
	dat1 <- subset(maoz.dat,maoz.dat$year==maoz.dat$year[i])#
	dat1mid <- subset(dat1,dat1$dichmid20==1)#
	datc1 <- subset(dat1mid,as.numeric(dat1mid$statea==maoz.dat$statea[i])+as.numeric(dat1mid$stateb==maoz.dat$statea[i]) > 0)#
	datc2 <- subset(dat1mid,as.numeric(dat1mid$statea==maoz.dat$stateb[i])+as.numeric(dat1mid$stateb==maoz.dat$stateb[i]) > 0)#
	us1 <- unique(c(datc1$statea,datc1$stateb))#
	if(is.element(maoz.dat$statea[i],us1)) us1 <- us1[-which(us1==maoz.dat$statea[i])]#
	if(is.element(maoz.dat$stateb[i],us1)) us1 <- us1[-which(us1==maoz.dat$stateb[i])]#
	us2 <- unique(c(datc2$statea,datc2$stateb))#
	if(is.element(maoz.dat$stateb[i],us2)) us2 <- us2[-which(us2==maoz.dat$stateb[i])]#
	if(is.element(maoz.dat$statea[i],us2)) us2 <- us2[-which(us2==maoz.dat$statea[i])]#
	if(min(c(length(us1),length(us2)))>0){#
		tchange[i] <- length(intersect(us1,us2))#
		tschange[i] <- length(us1)+length(us2)#
	}#
}#
#
### Make Lagged MID#
#
# Code in maoz.dat#
#
mdcode <- paste(maoz.dat$year,"#",maoz.dat$statea,"#",maoz.dat$stateb,sep="")#
#
# mids data#
#
mids <- subset(maoz.dat,maoz.dat$dichmid20==1)#
#
lmidsc <- paste(mids$year+1,"#",mids$statea,"#",mids$stateb,sep="")#
#
lmidsc2 <- paste(mids$year+1,"#",mids$stateb,"#",mids$statea,sep="")#
#
mm1 <- match(mdcode,lmidsc)#
#
mm2 <- match(mdcode,lmidsc2) #
#
lmid <- 1-as.numeric(is.na(mm1)+is.na(mm2) ==2)#
#
whichab <- function(x,y){#
#
return(which(y==x))#
#
}#
##### Bootstrap year-by-year, with and without lagged conflict #
#
maoz.dat <- cbind(maoz.dat,tchange,tschange,lmid)#
#
m <- 1000#
set.seed(10)#
uyr <- unique(maoz.dat$year)#
coef.triad <- NULL#
coef.triad.l <- NULL#
for(i in 1:1000){#
	yrsi <- sample(uyr,length(uyr),rep=T)#
	yrsil <- as.list(yrsi)#
	inds <- unlist(lapply(yrsil,whichab,y=maoz.dat$year))#
	datai <- maoz.dat[inds,]#
	mod.triad <- glm(dichmid20~midspline3+minreg302+caprat301+distance+integcorrse+tchange+tschange, data=datai,family=binomial)#
	mod.triadl <- glm(dichmid20~midspline3+minreg302+caprat301+distance+integcorrse+tchange+tschange+lmid, data=datai,family=binomial)#
	coef.triad <- rbind(coef.triad,rbind(mod.triad$coefficients))#
	coef.triad.l <- rbind(coef.triad.l,rbind(mod.triadl$coefficients))#
}#
#
dput(list(coef=coef.triad,coefl = coef.triad.l),"/Users/bbd5087/Dropbox/professional/Research/Published/ERGM/Data/bootergmDep.txt")
library(foreign)#
library(sna)#
#
maoz <- read.dta("/Users/bbd5087/Dropbox/professional/Research/Published/ERGM/Data/jcr2006/jcr2006replication.dta")#
maoz.vars <- c("year","statea","stateb","newmid20","dichmid20","midspline2","midspline3","minreg302","caprat301","distance","integcorrse","atopsecorr","igosecorr")#
maoz.dat <- NULL#
for(i in maoz.vars){#
	maoz.dat <- cbind(maoz.dat,maoz[,which(names(maoz)==i)])#
}#
#
maoz.dat <- data.frame(maoz.dat)#
names(maoz.dat) <- maoz.vars#
#
maoz.dat <- na.omit(maoz.dat)#
#
maozrep <- glm(dichmid20~midspline2+minreg302+caprat301+distance+atopsecorr+igosecorr+tradesecorr, data=maoz.dat,family=binomial)#
#
# Create triadic changestats#
#
tchange <- numeric(nrow(maoz.dat))#
tschange <- numeric(nrow(maoz.dat))#
#
for(i in 1:nrow(maoz.dat)){#
	dat1 <- subset(maoz.dat,maoz.dat$year==maoz.dat$year[i])#
	dat1mid <- subset(dat1,dat1$dichmid20==1)#
	datc1 <- subset(dat1mid,as.numeric(dat1mid$statea==maoz.dat$statea[i])+as.numeric(dat1mid$stateb==maoz.dat$statea[i]) > 0)#
	datc2 <- subset(dat1mid,as.numeric(dat1mid$statea==maoz.dat$stateb[i])+as.numeric(dat1mid$stateb==maoz.dat$stateb[i]) > 0)#
	us1 <- unique(c(datc1$statea,datc1$stateb))#
	if(is.element(maoz.dat$statea[i],us1)) us1 <- us1[-which(us1==maoz.dat$statea[i])]#
	if(is.element(maoz.dat$stateb[i],us1)) us1 <- us1[-which(us1==maoz.dat$stateb[i])]#
	us2 <- unique(c(datc2$statea,datc2$stateb))#
	if(is.element(maoz.dat$stateb[i],us2)) us2 <- us2[-which(us2==maoz.dat$stateb[i])]#
	if(is.element(maoz.dat$statea[i],us2)) us2 <- us2[-which(us2==maoz.dat$statea[i])]#
	if(min(c(length(us1),length(us2)))>0){#
		tchange[i] <- length(intersect(us1,us2))#
		tschange[i] <- length(us1)+length(us2)#
	}#
}#
#
### Make Lagged MID#
#
# Code in maoz.dat#
#
mdcode <- paste(maoz.dat$year,"#",maoz.dat$statea,"#",maoz.dat$stateb,sep="")#
#
# mids data#
#
mids <- subset(maoz.dat,maoz.dat$dichmid20==1)#
#
lmidsc <- paste(mids$year+1,"#",mids$statea,"#",mids$stateb,sep="")#
#
lmidsc2 <- paste(mids$year+1,"#",mids$stateb,"#",mids$statea,sep="")#
#
mm1 <- match(mdcode,lmidsc)#
#
mm2 <- match(mdcode,lmidsc2) #
#
lmid <- 1-as.numeric(is.na(mm1)+is.na(mm2) ==2)#
#
whichab <- function(x,y){#
#
return(which(y==x))#
#
}#
##### Bootstrap year-by-year, with and without lagged conflict #
#
maoz.dat <- cbind(maoz.dat,tchange,tschange,lmid)#
#
m <- 1000#
set.seed(10)#
uyr <- unique(maoz.dat$year)#
coef.triad <- NULL#
coef.triad.l <- NULL#
for(i in 1:1000){#
	yrsi <- sample(uyr,length(uyr),rep=T)#
	yrsil <- as.list(yrsi)#
	inds <- unlist(lapply(yrsil,whichab,y=maoz.dat$year))#
	datai <- maoz.dat[inds,]#
	mod.triad <- glm(dichmid20~midspline3+minreg302+caprat301+distance+integcorrse+tchange+tschange, data=datai,family=binomial)#
	mod.triadl <- glm(dichmid20~midspline3+minreg302+caprat301+distance+integcorrse+tchange+tschange+lmid, data=datai,family=binomial)#
	coef.triad <- rbind(coef.triad,rbind(mod.triad$coefficients))#
	coef.triad.l <- rbind(coef.triad.l,rbind(mod.triadl$coefficients))#
}#
#
dput(list(coef=coef.triad,coefl = coef.triad.l),"/Users/bbd5087/Dropbox/professional/Research/Published/ERGM/Data/bootergmDep.txt")
library(foreign)#
library(sna)#
#
maoz <- read.dta("/Users/bbd5087/Dropbox/professional/Research/Published/ERGM/Data/jcr2006/jcr2006replication.dta")#
maoz.vars <- c("year","statea","stateb","newmid20","dichmid20","midspline2","midspline3","minreg302","caprat301","distance","integcorrse","atopsecorr","igosecorr","tradescorr")#
maoz.dat <- NULL#
for(i in maoz.vars){#
	maoz.dat <- cbind(maoz.dat,maoz[,which(names(maoz)==i)])#
}#
#
maoz.dat <- data.frame(maoz.dat)#
names(maoz.dat) <- maoz.vars#
#
maoz.dat <- na.omit(maoz.dat)#
#
maozrep <- glm(dichmid20~midspline2+minreg302+caprat301+distance+atopsecorr+igosecorr+tradesecorr, data=maoz.dat,family=binomial)#
#
# Create triadic changestats#
#
tchange <- numeric(nrow(maoz.dat))#
tschange <- numeric(nrow(maoz.dat))#
#
for(i in 1:nrow(maoz.dat)){#
	dat1 <- subset(maoz.dat,maoz.dat$year==maoz.dat$year[i])#
	dat1mid <- subset(dat1,dat1$dichmid20==1)#
	datc1 <- subset(dat1mid,as.numeric(dat1mid$statea==maoz.dat$statea[i])+as.numeric(dat1mid$stateb==maoz.dat$statea[i]) > 0)#
	datc2 <- subset(dat1mid,as.numeric(dat1mid$statea==maoz.dat$stateb[i])+as.numeric(dat1mid$stateb==maoz.dat$stateb[i]) > 0)#
	us1 <- unique(c(datc1$statea,datc1$stateb))#
	if(is.element(maoz.dat$statea[i],us1)) us1 <- us1[-which(us1==maoz.dat$statea[i])]#
	if(is.element(maoz.dat$stateb[i],us1)) us1 <- us1[-which(us1==maoz.dat$stateb[i])]#
	us2 <- unique(c(datc2$statea,datc2$stateb))#
	if(is.element(maoz.dat$stateb[i],us2)) us2 <- us2[-which(us2==maoz.dat$stateb[i])]#
	if(is.element(maoz.dat$statea[i],us2)) us2 <- us2[-which(us2==maoz.dat$statea[i])]#
	if(min(c(length(us1),length(us2)))>0){#
		tchange[i] <- length(intersect(us1,us2))#
		tschange[i] <- length(us1)+length(us2)#
	}#
}#
#
### Make Lagged MID#
#
# Code in maoz.dat#
#
mdcode <- paste(maoz.dat$year,"#",maoz.dat$statea,"#",maoz.dat$stateb,sep="")#
#
# mids data#
#
mids <- subset(maoz.dat,maoz.dat$dichmid20==1)#
#
lmidsc <- paste(mids$year+1,"#",mids$statea,"#",mids$stateb,sep="")#
#
lmidsc2 <- paste(mids$year+1,"#",mids$stateb,"#",mids$statea,sep="")#
#
mm1 <- match(mdcode,lmidsc)#
#
mm2 <- match(mdcode,lmidsc2) #
#
lmid <- 1-as.numeric(is.na(mm1)+is.na(mm2) ==2)#
#
whichab <- function(x,y){#
#
return(which(y==x))#
#
}#
##### Bootstrap year-by-year, with and without lagged conflict #
#
maoz.dat <- cbind(maoz.dat,tchange,tschange,lmid)#
#
m <- 1000#
set.seed(10)#
uyr <- unique(maoz.dat$year)#
coef.triad <- NULL#
coef.triad.l <- NULL#
for(i in 1:1000){#
	yrsi <- sample(uyr,length(uyr),rep=T)#
	yrsil <- as.list(yrsi)#
	inds <- unlist(lapply(yrsil,whichab,y=maoz.dat$year))#
	datai <- maoz.dat[inds,]#
	mod.triad <- glm(dichmid20~midspline3+minreg302+caprat301+distance+integcorrse+tchange+tschange, data=datai,family=binomial)#
	mod.triadl <- glm(dichmid20~midspline3+minreg302+caprat301+distance+integcorrse+tchange+tschange+lmid, data=datai,family=binomial)#
	coef.triad <- rbind(coef.triad,rbind(mod.triad$coefficients))#
	coef.triad.l <- rbind(coef.triad.l,rbind(mod.triadl$coefficients))#
}#
#
dput(list(coef=coef.triad,coefl = coef.triad.l),"/Users/bbd5087/Dropbox/professional/Research/Published/ERGM/Data/bootergmDep.txt")
ncol(maoz.dat)
length(maoz.vars)
maoz <- read.dta("/Users/bbd5087/Dropbox/professional/Research/Published/ERGM/Data/jcr2006/jcr2006replication.dta")#
maoz.vars <- c("year","statea","stateb","newmid20","dichmid20","midspline2","midspline3","minreg302","caprat301","distance","integcorrse","atopsecorr","igosecorr","tradescorr")#
maoz.dat <- NULL#
for(i in maoz.vars){#
	maoz.dat <- cbind(maoz.dat,maoz[,which(names(maoz)==i)])#
}
length(maoz.vars)
ncol(maoz.dat)
library(foreign)#
library(sna)#
#
maoz <- read.dta("/Users/bbd5087/Dropbox/professional/Research/Published/ERGM/Data/jcr2006/jcr2006replication.dta")#
maoz.vars <- c("year","statea","stateb","newmid20","dichmid20","midspline2","midspline3","minreg302","caprat301","distance","integcorrse","atopsecorr","igosecorr","tradesecorr")#
maoz.dat <- NULL#
for(i in maoz.vars){#
	maoz.dat <- cbind(maoz.dat,maoz[,which(names(maoz)==i)])#
}#
#
maoz.dat <- data.frame(maoz.dat)#
names(maoz.dat) <- maoz.vars#
#
maoz.dat <- na.omit(maoz.dat)#
#
maozrep <- glm(dichmid20~midspline2+minreg302+caprat301+distance+atopsecorr+igosecorr+tradesecorr, data=maoz.dat,family=binomial)#
#
# Create triadic changestats#
#
tchange <- numeric(nrow(maoz.dat))#
tschange <- numeric(nrow(maoz.dat))#
#
for(i in 1:nrow(maoz.dat)){#
	dat1 <- subset(maoz.dat,maoz.dat$year==maoz.dat$year[i])#
	dat1mid <- subset(dat1,dat1$dichmid20==1)#
	datc1 <- subset(dat1mid,as.numeric(dat1mid$statea==maoz.dat$statea[i])+as.numeric(dat1mid$stateb==maoz.dat$statea[i]) > 0)#
	datc2 <- subset(dat1mid,as.numeric(dat1mid$statea==maoz.dat$stateb[i])+as.numeric(dat1mid$stateb==maoz.dat$stateb[i]) > 0)#
	us1 <- unique(c(datc1$statea,datc1$stateb))#
	if(is.element(maoz.dat$statea[i],us1)) us1 <- us1[-which(us1==maoz.dat$statea[i])]#
	if(is.element(maoz.dat$stateb[i],us1)) us1 <- us1[-which(us1==maoz.dat$stateb[i])]#
	us2 <- unique(c(datc2$statea,datc2$stateb))#
	if(is.element(maoz.dat$stateb[i],us2)) us2 <- us2[-which(us2==maoz.dat$stateb[i])]#
	if(is.element(maoz.dat$statea[i],us2)) us2 <- us2[-which(us2==maoz.dat$statea[i])]#
	if(min(c(length(us1),length(us2)))>0){#
		tchange[i] <- length(intersect(us1,us2))#
		tschange[i] <- length(us1)+length(us2)#
	}#
}#
#
### Make Lagged MID#
#
# Code in maoz.dat#
#
mdcode <- paste(maoz.dat$year,"#",maoz.dat$statea,"#",maoz.dat$stateb,sep="")#
#
# mids data#
#
mids <- subset(maoz.dat,maoz.dat$dichmid20==1)#
#
lmidsc <- paste(mids$year+1,"#",mids$statea,"#",mids$stateb,sep="")#
#
lmidsc2 <- paste(mids$year+1,"#",mids$stateb,"#",mids$statea,sep="")#
#
mm1 <- match(mdcode,lmidsc)#
#
mm2 <- match(mdcode,lmidsc2) #
#
lmid <- 1-as.numeric(is.na(mm1)+is.na(mm2) ==2)#
#
whichab <- function(x,y){#
#
return(which(y==x))#
#
}#
##### Bootstrap year-by-year, with and without lagged conflict #
#
maoz.dat <- cbind(maoz.dat,tchange,tschange,lmid)#
#
m <- 1000#
set.seed(10)#
uyr <- unique(maoz.dat$year)#
coef.triad <- NULL#
coef.triad.l <- NULL#
for(i in 1:1000){#
	yrsi <- sample(uyr,length(uyr),rep=T)#
	yrsil <- as.list(yrsi)#
	inds <- unlist(lapply(yrsil,whichab,y=maoz.dat$year))#
	datai <- maoz.dat[inds,]#
	mod.triad <- glm(dichmid20~midspline3+minreg302+caprat301+distance+integcorrse+tchange+tschange, data=datai,family=binomial)#
	mod.triadl <- glm(dichmid20~midspline3+minreg302+caprat301+distance+integcorrse+tchange+tschange+lmid, data=datai,family=binomial)#
	coef.triad <- rbind(coef.triad,rbind(mod.triad$coefficients))#
	coef.triad.l <- rbind(coef.triad.l,rbind(mod.triadl$coefficients))#
}#
#
dput(list(coef=coef.triad,coefl = coef.triad.l),"/Users/bbd5087/Dropbox/professional/Research/Published/ERGM/Data/bootergmDep.txt")
?optim
loglik_ergm
#========section I============#
#Initialize the sample network provided in the question#
sample_net <- matrix(#
  c(0,1,1,1,1,#
    1,0,1,0,0,#
    1,0,0,0,0,#
    1,0,0,0,0,#
    0,0,0,0,0),#
  nrow = 5,#
  ncol = 5,#
  byrow = TRUE#
)#
edge <- function(net_mat){#
  # Compute how many edges are in the network.#
  ##
  # Args:#
  #   net_mat: the network to compute its edge number.#
  # Returns:#
  #   the number of edges in the network net_mat.#
  edge_value = sum(net_mat)#
  return(edge_value)#
}#
#test#
edge(sample_net)#
dyad <- function(net_mat){#
  # Compute how many mutual dyads are in the network.#
  ##
  # Args:#
  #   net_mat: the network to compute its mutual dyads number.#
  # Returns:#
  #   the number of mutual dyads in the network net_mat#
  n <- nrow(net_mat)#
  dyad_value = 0#
  for (i in 2:n){#
    for (j in 1:i){#
      if (net_mat[i,j] == 1 && net_mat[i,j] == net_mat[j,i]){#
        dyad_value = dyad_value + 1#
      }#
    }#
  }#
  return(dyad_value)#
}#
#test#
dyad(sample_net)#
net_denom_bf <- function(par, net_mat){#
  # The function that computes the denominator part of the likelihood function#
  # This is a brute force way to compute, very inefficient.#
  # It compute all the possible networks.#
  # When the number of nodes become large, it takes really long time#
  # Args:#
  #   par: theta_1 and theta_2, coefficients for edge and dyad, respectively#
  #   net_mat: the input network.#
  # Return:#
  #   return the denominater in the likelihood function. Needs to use log() for log-likelihood#
  nodes <- nrow(net_mat)#
  total <- 0#
  for (i in 0:2^(nodes * nodes)){#
    #sim_mat will iterate through all possible networks#
    sim_mat = matrix(as.integer(intToBits(i)), nodes, nodes)#
    if(sum(diag(sim_mat)) == 0){#
      total = total + exp(edge(sim_mat) * par[1] + dyad(sim_mat) * par[2])#
    }#
  }#
  return(total)#
}#
loglik_ergm <- function(par, net_mat){#
  # The function to compute log-likelihood of ERGM#
  # Args:#
  #   par: theta_1 and theta_2, coefficients for edge and dyad, respectively#
  #   net_mat: the input network#
  # Return:#
  #   The negative value of log-likelihood for ERGM#
  # -(par[1] * edge(net_mat) + par[2] * dyad(net_mat) - log(net_denom_bf(par, net_mat)))#
  -(par[1] * edge(net_mat) + par[2] * dyad(net_mat) + log(net_denom_bf(par, net_mat)))#
}
loglik_ergm
loglik_ergm(c(0,0),sample_net)
warnings()
intToBits(10)
intToBits(1000)
intToBits(10000)
intToBits(0)
net_denom_bf <- function(par, net_mat){#
  # The function that computes the denominator part of the likelihood function#
  # This is a brute force way to compute, very inefficient.#
  # It compute all the possible networks.#
  # When the number of nodes become large, it takes really long time#
  # Args:#
  #   par: theta_1 and theta_2, coefficients for edge and dyad, respectively#
  #   net_mat: the input network.#
  # Return:#
  #   return the denominater in the likelihood function. Needs to use log() for log-likelihood#
  nodes <- nrow(net_mat)#
  total <- 0#
  for (i in 0:2^(nodes * nodes)){#
    #sim_mat will iterate through all possible networks#
    sim_mat = matrix(as.integer(intToBits(i)), nodes, nodes)#
    if(sum(diag(sim_mat)) == 0){#
      total = total + exp(edge(sim_mat) * par[1] + dyad(sim_mat) * par[2])#
    }#
    if(i/100==round(i/100)) print(i)#
  }#
  return(total)#
}
loglik_ergm(c(0,0),sample_net)
2^25
2^25/1000000
2^25/10000000
net_denom_bf <- function(par, net_mat){#
  # The function that computes the denominator part of the likelihood function#
  # This is a brute force way to compute, very inefficient.#
  # It compute all the possible networks.#
  # When the number of nodes become large, it takes really long time#
  # Args:#
  #   par: theta_1 and theta_2, coefficients for edge and dyad, respectively#
  #   net_mat: the input network.#
  # Return:#
  #   return the denominater in the likelihood function. Needs to use log() for log-likelihood#
  nodes <- nrow(net_mat)#
  total <- 0#
  for (i in 0:2^(nodes * nodes)){#
    #sim_mat will iterate through all possible networks#
    sim_mat = matrix(as.integer(intToBits(i)), nodes, nodes)#
    if(sum(diag(sim_mat)) == 0){#
      total = total + exp(edge(sim_mat) * par[1] + dyad(sim_mat) * par[2])#
    }#
    if(i/10000==round(i/10000)) print(i)#
  }#
  return(total)#
}
system.time(loglik_ergm(c(0,0),sample_net))
net_denom_bf <- function(par, net_mat){#
  # The function that computes the denominator part of the likelihood function#
  # This is a brute force way to compute, very inefficient.#
  # It compute all the possible networks.#
  # When the number of nodes become large, it takes really long time#
  # Args:#
  #   par: theta_1 and theta_2, coefficients for edge and dyad, respectively#
  #   net_mat: the input network.#
  # Return:#
  #   return the denominater in the likelihood function. Needs to use log() for log-likelihood#
  nodes <- nrow(net_mat)#
  total <- 0#
  for (i in 0:2^(nodes * nodes)){#
    #sim_mat will iterate through all possible networks#
    sim_mat = matrix(as.integer(intToBits(i)), nodes, nodes)#
    if(sum(diag(sim_mat)) == 0){#
      total = total + exp(edge(sim_mat) * par[1] + dyad(sim_mat) * par[2])#
    }#
    if(i/100000==round(i/100000)) print(i)#
  }#
  return(total)#
}
system.time(loglik_ergm(c(0,0),sample_net))
1628.917/60
?ergm
library(ergm)
?ergm
require(RcppGSL)
rdirichlet_cpp(1,c(3,2,2))
require(MCMCpack)
install.packages("MCMCpack")
require(MCMCpack)
library(MCMCpack)
rdirichlet
rdirichlet(1,c(3,2,2))
rdirichlet(1,c(3,2,2))
rdirichlet(1,c(3,2,2))
word.type.sampler <- function(mvec,ntokens=200,samples=1000,backwards_inner=1){#
require(MCMCpack)#
#
forwardSample <- NULL#
#
backwardSample <- NULL#
#
for(i in 1:samples){#
#
    # Taking a forward sample#
    theta.d = rdirichlet(1, mvec)#
    forwardText = rmultinom(ntokens, theta.d)#
    forwardSample <- cbind(forwardSample,forwardText)#
#
    # taking a backward sample#
    backwardText <- forwardText#
    for(j in 1:backwards_inner){#
        phi <- numeric(length(mvec))#
        for(p in 1:length(mvec)){#
            phi[p] <- (sum(backwardText==p)+mvec[p])/(length(backwardText)+sum(mvec))#
        }#
        backwardText <- rmultinom(ntokens,phi)#
    }#
    backwardSample <- cbind(backwardSample,backwardText)#
}#
list(forward=forwardSample,backward=backwardSample)#
#
}
word.type.sampler <- function(mvec=1:4,ntokens=200,samples=1000,backwards_inner=1){#
require(MCMCpack)#
#
forwardSample <- NULL#
#
backwardSample <- NULL#
#
for(i in 1:samples){#
#
    # Taking a forward sample#
    theta.d = rdirichlet(1, mvec)#
    forwardText = rmultinom(ntokens, theta.d)#
    forwardSample <- cbind(forwardSample,forwardText)#
#
    # taking a backward sample#
    backwardText <- forwardText#
    for(j in 1:backwards_inner){#
        phi <- numeric(length(mvec))#
        for(p in 1:length(mvec)){#
            phi[p] <- (sum(backwardText==p)+mvec[p])/(length(backwardText)+sum(mvec))#
        }#
        backwardText <- rmultinom(ntokens,phi)#
    }#
    backwardSample <- cbind(backwardSample,backwardText)#
}#
list(forward=forwardSample,backward=backwardSample)#
#
}
GIRSamples <- word.type.sampler(backwards_inner=3)
?rmultinom
rmultinom(10, size = 12, prob = c(0.1,0.2,0.8))
mvec=1:4,ntokens=200,samples=1000,backwards_inner=1
mvec=1:4;ntokens=200;samples=1000;backwards_inner=1
theta.d = rdirichlet(1, mvec)
forwardText = c(rmultinom(1,ntokens, theta.d))
forwardText
word.type.sampler <- function(mvec=1:4,ntokens=200,samples=1000,backwards_inner=1){#
require(MCMCpack)#
#
forwardSample <- NULL#
#
backwardSample <- NULL#
#
for(i in 1:samples){#
#
    # Taking a forward sample#
    theta.d = rdirichlet(1, mvec)#
    forwardText = c(rmultinom(1,ntokens, theta.d))#
    forwardSample <- cbind(forwardSample,forwardText)#
#
    # taking a backward sample#
    backwardText <- forwardText#
    for(j in 1:backwards_inner){#
        phi <- numeric(length(mvec))#
        for(p in 1:length(mvec)){#
            phi[p] <- (backwardText[p]+mvec[p])/(length(backwardText)+sum(mvec))#
        }#
        backwardText <- c(rmultinom(1,ntokens, theta.d))#
    }#
    backwardSample <- cbind(backwardSample,backwardText)#
}#
list(forward=forwardSample,backward=backwardSample)#
#
}#
#
GIRSamples <- word.type.sampler(backwards_inner=3)
GIRSamples
boxplot(GIRSamples$forward[1,],GIRSamples$backward[1,])
ppplot <- function(x1,x2,thin=1){#
    # setting thin uses every thinth unique value in the plotting#
    # find all unique values across the two vectors#
    uniqueValues <- sort(unique(c(x1,x2)))#
    # thinning#
    uniqueValues <- uniqueValues[round(seq(1,length(uniqueValues),by=thin))]#
    # create vectors in which to store empirical quantiles#
    qx1 <- numeric(length(uniqueValues))#
    qx2 <- numeric(length(uniqueValues))#
    # loop (could be paralellized) to calculate empirical quantile position#
    # of each unique value in each vector#
    for(i in 1:length(uniqueValues)){#
        qx1[i] <- mean(x1 <= uniqueValues[i])#
        qx2[i] <- mean(x2 <= uniqueValues[i])#
    }#
    plot(qx1,qx2,type="n",ylim=c(0,1),xlim=c(0,1),ylab="quantile of x2",xlab="quantile of x1")#
    abline(0,1,lty=2,col="grey60")#
    points(qx1,qx2,cex=.6,col="blue",pch=4)#
    lines(qx1,qx2)#
}
GIRSamples <- word.type.sampler(backwards_inner=30)
# Nsamp, nDocs = 5, node = 1:4, vocabulary =  c("hi", "hello","bye", "mine", "what"),  nIP = 2, K = 4, nwords = 4, alpha = 2, mvec = rep(1/4, 4), betas = 2, nvec = rep(1/5, 5),   prior.b.mean = c(-3, rep(0, 6)), prior.b.var = diag(7), prior.eta = c(0, 1), sigma_Q = 0.25, niters = c(1, 1, 1, 50, 0, 1), netstat = "dyadic", generate_PP_plots = TRUE, seed = 1#
word.type.sampler <- function(mvec=1:4,ntokens=200,samples=1000,backwards_inner=1){#
require(MCMCpack)#
#
forwardSample <- NULL#
#
backwardSample <- NULL#
#
for(i in 1:samples){#
#
    # Taking a forward sample#
    theta.d = rdirichlet(1, mvec)#
    forwardText = c(rmultinom(1,ntokens, theta.d))#
    forwardSample <- cbind(forwardSample,forwardText)#
#
    # taking a backward sample#
    backwardText <- forwardText#
    for(j in 1:backwards_inner){#
        phi <- numeric(length(mvec))#
        for(p in 1:length(mvec)){#
            phi[p] <- (backwardText[p]+mvec[p])/(length(backwardText)+sum(mvec))#
        }#
        backwardText <- c(rmultinom(1,ntokens, phi))#
    }#
    backwardSample <- cbind(backwardSample,backwardText)#
}#
list(forward=forwardSample,backward=backwardSample)#
#
}#
#
GIRSamples <- word.type.sampler(backwards_inner=30)
ppplot(GIRSamples$forward[1,],GIRSamples$backward[1,])
GIRSamples <- word.type.sampler(backwards_inner=3)#
#
ppplot(GIRSamples$forward[1,],GIRSamples$backward[1,])
GIRSamples <- word.type.sampler(backwards_inner=300)#
#
ppplot(GIRSamples$forward[1,],GIRSamples$backward[1,])
var(GIRSamples$forward[1,],GIRSamples$backward[1,])
sd(GIRSamples$forward[1,],GIRSamples$backward[1,])
sd(GIRSamples$forward[1,])
sd(GIRSamples$backward[1,])
mean(GIRSamples$backward[1,])
mean(GIRSamples$forward[1,])
mean(GIRSamples$forward[2,])
mean(GIRSamples$forward[2,])
mean(GIRSamples$backward[2,])
t.test(GIRSamples$forward[1,],GIRSamples$backward[1,])
GIRSamples <- word.type.sampler(backwards_inner=1)#
#
ppplot(GIRSamples$forward[1,],GIRSamples$backward[1,])
word.type.sampler <- function(mvec=1:4,ntokens=200,samples=100){#
require(MCMCpack)#
#
forwardSample <- NULL#
#
backwardSample <- NULL#
#
for(i in 1:samples){#
#
    # Taking a forward sample#
    theta.d = rdirichlet(1, mvec)#
    forwardText = c(rmultinom(1,ntokens, theta.d))#
    forwardSample <- cbind(forwardSample,forwardText)#
#
    # taking a backward sample#
    backwardText <- rep(0,length(mvec))#
    for(j in 1:ntokens){#
        phi <- numeric(length(mvec))#
        for(p in 1:length(mvec)){#
            phi[p] <- (backwardText[p]+mvec[p])/(sum(backwardText)+sum(mvec))#
        }#
        backwardText <- backwardText + c(rmultinom(1,1, phi))#
    }#
    backwardSample <- cbind(backwardSample,backwardText)#
}#
list(forward=forwardSample,backward=backwardSample)#
#
}
GIRSamples <- word.type.sampler()
ppplot(GIRSamples$forward[1,],GIRSamples$backward[1,])
mvec=1:4;ntokens=200;samples=100
require(MCMCpack)#
#
forwardSample <- NULL#
#
backwardSample <- NULL#
#
for(i in 1:samples){#
#
    # Taking a forward sample#
    theta.d = rdirichlet(1, mvec)#
    forwardText = c(rmultinom(1,ntokens, theta.d))#
    forwardSample <- cbind(forwardSample,forwardText)#
#
    # taking a backward sample#
    backwardText <- rep(0,length(mvec))#
    for(j in 1:ntokens){#
        phi <- numeric(length(mvec))#
        for(p in 1:length(mvec)){#
            phi[p] <- (backwardText[p]+mvec[p])/(sum(backwardText)+sum(mvec))#
        }#
        backwardText <- backwardText + c(rmultinom(1,1, phi))#
    }#
    backwardSample <- cbind(backwardSample,backwardText)#
}
phi
t.test(GIRSamples$forward[1,],GIRSamples$backward[1,])
ppplot(GIRSamples$forward[1,],GIRSamples$backward[1,])
GIRSamples <- word.type.sampler(samples=1000)#
#
ppplot(GIRSamples$forward[1,],GIRSamples$backward[1,])
GIRSamples <- word.type.sampler(samples=10000)#
#
ppplot(GIRSamples$forward[1,],GIRSamples$backward[1,])
ppplot(GIRSamples$forward[1,],GIRSamples$backward[1,],thin=5)
GIRSamples <- word.type.sampler(samples=100000)#
#
ppplot(GIRSamples$forward[1,],GIRSamples$backward[1,],thin=5)
ppplot(GIRSamples$forward[2,],GIRSamples$backward[2,],thin=5)
ppplot(GIRSamples$forward[4,],GIRSamples$backward[4,],thin=5)
?ergm
library(ergm)
?ergm
?ergm
?network
m <- matrix(rbinom(25,1,.4),5,5)#
diag(m) <- 0#
g <- network(m, directed=FALSE)#
summary(g)
m <- matrix(runif(25,0,1),5,5)#
diag(m) <- 0#
g <- network(m, directed=FALSE)#
summary(g)
setwd("~/Box Sync/Box/Research/FDI_IGERT_H/Code/TERGM")
as.matrix(g,"edgelis")
as.matrix(g,"edgelist")
# clear workspace#
rm(list=ls())#
#
set.seed(19)#
#
#setwd(dirname(rstudioapi::getActiveDocumentContext()$path))#
# libraries#
library(ergm.count)#
library(network)#
library(igraph)#
# load data#
#
trade_openess <- read.csv("trade_openness.csv", stringsAsFactors=FALSE)#
row.names(trade_openess) <- trade_openess[,1]#
#trade_openess <- data.frame(trade_openess[,-1])#
polity <- read.csv("polity.csv", stringsAsFactors=FALSE)#
row.names(polity) <- polity[,1]#
#polity <- data.frame(polity[,-1])#
dist <- read.csv("dist.csv", stringsAsFactors=FALSE)#
row.names(dist) <- dist[,1]#
dist <- data.frame(dist[,-1])#
mass <- read.csv("mass.csv", stringsAsFactors=FALSE)#
row.names(mass) <- mass[,1]#
mass <- data.frame(mass[,-1])#
fdi_lag <- read.csv("fdi_lag.csv", stringsAsFactors=FALSE)#
row.names(fdi_lag) <- fdi_lag[,1]#
fdi_lag <- data.frame(fdi_lag[,-1])#
fdi <- read.csv("fdi.csv", stringsAsFactors=FALSE)#
row.names(fdi) <- fdi[,1]#
fdi <- data.frame(fdi[,-1])
install.packages("igraph")
fdi[1:10,1:10]
fdi.net <- network(fdi,dir=T)
fdi.edgelist <- as.matrix(fdi.net,"edgelist")
fdi.edgelist[1:5,]
set.edge.attribute(fdi.net,fdi[fdi.edgelist])
set.edge.attribute(fdi.net,"fdi.value",fdi[fdi.edgelist])
get.edge.attribute(fdi.net,"fdi.value")
year <- substr(row.names(fdi),nchar(row.names(fdi))-3,nchar(row.names(fdi)))
table(year)
cbind(fdi.edgelist,get.edge.attribute(fdi.net,"fdi.value"))[1:20,]
fdi[38,1]
fdi[121,2]
fdi[120,3]
cbind(fdi.edgelist,get.edge.attribute(fdi.net,"fdi.value"))[180:200,]
cbind(fdi.edgelist,get.edge.attribute(fdi.net,"fdi.value"))[1000:1020,]
cbind(fdi.edgelist,get.edge.attribute(fdi.net,"fdi.value"))[2000:2020,]
fdi[178,156]
fdi[246,154]
?ergm
install.packages("ergm.count")
# clear workspace#
rm(list=ls())#
#
set.seed(19)#
#
#setwd(dirname(rstudioapi::getActiveDocumentContext()$path))#
# libraries#
library(ergm.count)#
library(network)#
#library(igraph)#
# load data#
#
trade_openess <- read.csv("trade_openness.csv", stringsAsFactors=FALSE)#
row.names(trade_openess) <- trade_openess[,1]#
#trade_openess <- data.frame(trade_openess[,-1])#
polity <- read.csv("polity.csv", stringsAsFactors=FALSE)#
row.names(polity) <- polity[,1]#
#polity <- data.frame(polity[,-1])#
dist <- read.csv("dist.csv", stringsAsFactors=FALSE)#
row.names(dist) <- dist[,1]#
dist <- data.frame(dist[,-1])#
mass <- read.csv("mass.csv", stringsAsFactors=FALSE)#
row.names(mass) <- mass[,1]#
mass <- data.frame(mass[,-1])#
fdi_lag <- read.csv("fdi_lag.csv", stringsAsFactors=FALSE)#
row.names(fdi_lag) <- fdi_lag[,1]#
fdi_lag <- data.frame(fdi_lag[,-1])#
fdi <- read.csv("fdi.csv", stringsAsFactors=FALSE)#
row.names(fdi) <- fdi[,1]#
fdi <- data.frame(fdi[,-1])#
#
year <- substr(row.names(fdi),nchar(row.names(fdi))-3,nchar(row.names(fdi)))#
for(i in 2:length(year)){#
    for(j in 1:(i-1)){#
        if(year[i]!=year[j]){#
            fdi[i,j] <- NA#
            fdi[j,i] <- NA#
        }#
    }#
}#
#
fdi.net <- network(fdi,dir=T)#
fdi.edgelist <- as.matrix(fdi.net,"edgelist")#
set.edge.attribute(fdi.net,"fdi.value",fdi[fdi.edgelist])#
set.vertex.attribute(fdi.net,"year",year)
